"""
LLMå†³ç­–å™¨ - åŸºäºé˜¿é‡Œç™¾ç‚¼å¹³å°
è´Ÿè´£è°ƒç”¨å¤§æ¨¡å‹è¿›è¡ŒAgenté€‰æ‹©å†³ç­–
"""
import json
from typing import Dict, Any, Optional
from openai import OpenAI
from src.core.events import OrchestratorContext, OrchestratorDecision

class LLMDecisionMaker:
    """LLMå†³ç­–å™¨"""
    
    def __init__(self, api_key: str, base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"):
        """
        åˆå§‹åŒ–LLMå†³ç­–å™¨
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
        """
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = "qwen-plus"  # ä½¿ç”¨é€šä¹‰åƒé—®Plusæ¨¡å‹
    
    def build_prompt(self, context: OrchestratorContext) -> str:
        """
        æ„å»ºå†³ç­–æç¤ºè¯
        
        Args:
            context: Orchestratorä¸Šä¸‹æ–‡
            
        Returns:
            å†³ç­–æç¤ºè¯
        """
        # æ„å»ºå¯ç”¨Agentsä¿¡æ¯
        agents_info = []
        for agent in context.available_agents:
            agent_desc = {
                "name": agent.name,
                "description": agent.description,
                "capabilities": agent.capabilities
            }
            agents_info.append(agent_desc)
        
        # æ„å»ºçŸ­æœŸè®°å¿†ï¼ˆå¯¹è¯å†å²ï¼‰
        conversation_history = []
        for memory in context.short_term_memories:
            conversation_history.append({
                "user": memory.query,
                "assistant": memory.response
            })
        
        # æ„å»ºé•¿æœŸè®°å¿†
        long_term_info = ""
        if context.long_term_memory:
            long_term_info = f"""
ç”¨æˆ·ç”»åƒå’Œåå¥½ï¼š
- æ‘˜è¦ï¼š{context.long_term_memory.summary}
- ç”¨æˆ·ä¿¡æ¯ï¼š{json.dumps(context.long_term_memory.user_profile, ensure_ascii=False, indent=2)}
- åå¥½è®¾ç½®ï¼š{json.dumps(context.long_term_memory.preferences, ensure_ascii=False, indent=2)}
"""
        
        # æ„å»ºç³»ç»ŸçŠ¶æ€
        system_states_info = []
        for state in context.system_states:
            system_states_info.append({
                "type": state.state_type,
                "data": state.state_data
            })
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è½¦è½½åŠ©æ‰‹çš„å†³ç­–ä¸­å¿ƒï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢å’Œå½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€‰æ‹©æœ€åˆé€‚çš„Agentæ¥å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚

**ç”¨æˆ·å½“å‰æŸ¥è¯¢ï¼š**
{context.input_query.query_content}

**å¯¹è¯å†å²ï¼š**
{json.dumps(conversation_history, ensure_ascii=False, indent=2)}

** ç”¨æˆ·ç”»åƒå’Œåå¥½ï¼š**
{long_term_info}

**å¯ç”¨çš„Agentsï¼š**
{json.dumps(agents_info, ensure_ascii=False, indent=2)}

**å†³ç­–è¦æ±‚ï¼š**
1. ä»”ç»†åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾
2. è€ƒè™‘å¯¹è¯å†å²å’Œç”¨æˆ·åå¥½
3. å‚è€ƒå½“å‰ç³»ç»ŸçŠ¶æ€
4. ä»å¯ç”¨çš„Agentsä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ª
5. å¦‚æœæŸ¥è¯¢æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ˆå¦‚ï¼šå¯¼èˆª+éŸ³ä¹+ç©ºè°ƒï¼‰ï¼Œæˆ–éœ€è¦å¤šæ­¥éª¤åè°ƒå®Œæˆï¼Œé€‰æ‹©"planner_agent"è¿›è¡Œä»»åŠ¡è§„åˆ’å’Œåè°ƒæ‰§è¡Œ
6. å¦‚æœç”¨æˆ·æŸ¥è¯¢ä¸æ˜ç¡®æˆ–æ— æ³•ç”±ä»»ä½•Agentå¤„ç†ï¼Œé€‰æ‹©"chat_agent"è¿›è¡Œé—²èŠå¯¹è¯

**è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼‰ï¼š**
{{
    "selected_agent": "agentåç§°",
    "confidence": 0.95,
    "reasoning": "é€‰æ‹©è¿™ä¸ªagentçš„è¯¦ç»†ç†ç”±",
    "parameters": {{
        "key1": "value1",
        "key2": "value2"
    }}
}}

è¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„å†³ç­–ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ã€‚
"""
        return prompt
    
    def make_decision(self, context: OrchestratorContext) -> OrchestratorDecision:
        """
        è¿›è¡Œå†³ç­–
        
        Args:
            context: Orchestratorä¸Šä¸‹æ–‡
            
        Returns:
            å†³ç­–ç»“æœ
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self.build_prompt(context)

            print("ğŸš€ è°ƒç”¨LLMè¿›è¡Œå†³ç­–...")
            print(f"Prompt:\n{prompt}\n")
            
            # è°ƒç”¨å¤§æ¨¡å‹
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å†³ç­–ç³»ç»Ÿï¼Œè´Ÿè´£åˆ†æç”¨æˆ·æ„å›¾å¹¶é€‰æ‹©åˆé€‚çš„Agentå¤„ç†è¯·æ±‚ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œä½¿è¾“å‡ºæ›´ç¡®å®š
                response_format={"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡º
            )
            
            # è§£æå“åº”
            response_text = completion.choices[0].message.content
            decision_data = json.loads(response_text)
            
            # æ„å»ºå†³ç­–ç»“æœ
            decision = OrchestratorDecision(
                selected_agent=decision_data.get("selected_agent", "chat_agent"),
                confidence=float(decision_data.get("confidence", 0.5)),
                reasoning=decision_data.get("reasoning", ""),
                parameters=decision_data.get("parameters", {}),
                metadata={
                    "model": self.model,
                    "tokens_used": completion.usage.total_tokens if completion.usage else 0
                }
            )
            
            return decision
            
        except Exception as e:
            print(f"âŒ LLMå†³ç­–å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å†³ç­–
            return OrchestratorDecision(
                selected_agent="chat_agent",
                confidence=0.1,
                reasoning=f"å†³ç­–å¤±è´¥ï¼Œé™çº§åˆ°é»˜è®¤Agent: {str(e)}",
                parameters={},
                metadata={"error": str(e)}
            )


class MockLLMDecisionMaker:
    """æ¨¡æ‹ŸLLMå†³ç­–å™¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿå†³ç­–å™¨"""
        self.decision_rules = {
            "éŸ³ä¹": "music_agent",
            "æ­Œ": "music_agent",
            "æ’­æ”¾": "music_agent",
            "å¯¼èˆª": "navigation_agent",
            "è·¯çº¿": "navigation_agent",
            "å»": "navigation_agent",
            "å¤©æ°”": "weather_agent",
            "æ¸©åº¦": "weather_agent",
            "è½¦çª—": "vehicle_control_agent",
            "ç©ºè°ƒ": "vehicle_control_agent",
            "åº§æ¤…": "vehicle_control_agent",
            "è½¦é—¨": "vehicle_control_agent",
            "æ‰“ç”µè¯": "phone_agent",
            "æ‹¨æ‰“": "phone_agent",
            "å‘¼å«": "phone_agent",
            "è”ç³»": "phone_agent",
            "ç”µè¯": "phone_agent",
            "å‘æ¶ˆæ¯": "phone_agent",
            "å‘çŸ­ä¿¡": "phone_agent"
        }
    
    def make_decision(self, context: OrchestratorContext) -> OrchestratorDecision:
        """
        è¿›è¡Œæ¨¡æ‹Ÿå†³ç­–
        
        Args:
            context: Orchestratorä¸Šä¸‹æ–‡
            
        Returns:
            å†³ç­–ç»“æœ
        """
        query = context.input_query.query_content.lower()
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        selected_agent = "chat_agent"
        confidence = 0.5
        reasoning = "åŸºäºå…³é”®è¯åŒ¹é…çš„é»˜è®¤å†³ç­–"
        
        for keyword, agent_name in self.decision_rules.items():
            if keyword in query:
                selected_agent = agent_name
                confidence = 0.9
                reasoning = f"æ£€æµ‹åˆ°å…³é”®è¯'{keyword}'ï¼Œé€‰æ‹©{agent_name}"
                break
        
        return OrchestratorDecision(
            selected_agent=selected_agent,
            confidence=confidence,
            reasoning=reasoning,
            parameters={},
            metadata={"mode": "mock"}
        )
