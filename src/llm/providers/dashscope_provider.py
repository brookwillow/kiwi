"""
DashScope Provider - é˜¿é‡Œäº‘é€šä¹‰åƒé—®

æ”¯æŒé€šè¿‡OpenAIå…¼å®¹APIè®¿é—®é˜¿é‡Œäº‘æ¨¡å‹
"""
from typing import List, Dict, Any, Optional, Iterator
from openai import OpenAI, APIError, AuthenticationError, RateLimitError
import time

from ..base_provider import BaseProvider
from ..types import (
    LLMRequest, LLMResponse, StreamChunk, EmbeddingResponse,
    TokenUsage, LLMMessage
)
from ..utils.errors import (
    LLMProviderError, LLMAPIError, LLMAuthenticationError,
    LLMRateLimitError, LLMTimeoutError, LLMNetworkError
)


class DashScopeProvider(BaseProvider):
    """é˜¿é‡Œäº‘DashScope Provider"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–DashScope Provider
        
        Args:
            config: Provideré…ç½®ï¼ŒåŒ…å«ï¼š
                - api_key: APIå¯†é’¥
                - base_url: APIåŸºç¡€URL
                - default_model: é»˜è®¤æ¨¡å‹å
                - timeout: è¶…æ—¶æ—¶é—´
                - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        super().__init__(config, "dashscope")
        
        self.api_key = config.get("api_key")
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")
        self.timeout = config.get("timeout", 30)
        self.max_retries = config.get("max_retries", 3)
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if self.api_key:
            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url,
                    timeout=self.timeout,
                    max_retries=self.max_retries
                )
                self._initialized = True
            except Exception as e:
                self.logger.error(f"åˆå§‹åŒ–DashScopeå®¢æˆ·ç«¯å¤±è´¥: {e}")
                self._initialized = False
        else:
            self.logger.warning("æœªæä¾›DashScope APIå¯†é’¥")
            self._initialized = False
    
    def chat_completion(self, request: LLMRequest) -> LLMResponse:
        """åŒæ­¥èŠå¤©å®Œæˆ"""
        if not self.is_available():
            raise LLMProviderError(
                provider=self.provider_name,
                message="Providerä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®"
            )
        
        self._log_request(request)
        
        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = request.to_openai_format()
            if not params.get("model"):
                raise LLMProviderError(
                    provider=self.provider_name,
                    message="å¿…é¡»æŒ‡å®šæ¨¡å‹åç§°"
                )
            
            # è¯¦ç»†æ—¥å¿—ï¼šæ‰“å°å‘é€ç»™APIçš„æ¶ˆæ¯åºåˆ—ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            self.logger.info(f"ğŸ“¤ å‘é€åˆ°DashScope APIçš„æ¶ˆæ¯åºåˆ— (å…±{len(params['messages'])}æ¡):")
            for i, msg in enumerate(params['messages']):
                role = msg.get('role', 'unknown')
                has_tool_calls = 'tool_calls' in msg
                has_tool_call_id = 'tool_call_id' in msg
                content_preview = msg.get('content', '')[:50] if msg.get('content') else '(ç©º)'
                
                log_line = f"  [{i+1}] role={role}"
                if has_tool_calls:
                    log_line += f" [æœ‰tool_calls: {len(msg['tool_calls'])}ä¸ª]"
                if has_tool_call_id:
                    log_line += f" [tool_call_id={msg['tool_call_id']}]"
                if not has_tool_calls and not has_tool_call_id:
                    log_line += f" - {content_preview}"
                
                self.logger.info(log_line)
            
            # è°ƒç”¨API
            start_time = time.time()
            response = self.client.chat.completions.create(**params)
            elapsed_time = time.time() - start_time
            
            # è§£æå“åº”
            choice = response.choices[0]
            message = choice.message
            content = message.content or ""
            
            # å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚å…³é—­æ€è€ƒæ¨¡å¼ï¼Œåˆ™è¿‡æ»¤ <think> æ ‡ç­¾
            # é€šä¹‰åƒé—®ç­‰æ¨¡å‹å¯èƒ½ä¼šåœ¨è¾“å‡ºä¸­åŒ…å« <think></think> æ ‡ç­¾
            enable_thinking = getattr(request, 'enable_thinking', True)
            if enable_thinking is False:
                import re
                # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹
                content = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL)
                # å¦‚æœæœ‰æœªé—­åˆçš„ <think> æ ‡ç­¾ï¼ˆå†…å®¹è¢«æˆªæ–­ï¼‰ï¼Œä¹Ÿç§»é™¤
                content = re.sub(r"<think>.*$", "", content, flags=re.DOTALL)
                content = content.strip()
                
                # å¦‚æœè¿‡æ»¤åå†…å®¹ä¸ºç©ºï¼Œè®°å½•è­¦å‘Š
                if not content:
                    self.logger.warning(f"è¿‡æ»¤<think>æ ‡ç­¾åå†…å®¹ä¸ºç©ºï¼ŒåŸå§‹å“åº”: {message.content[:100] if message.content else ''}")
            
            result = LLMResponse(
                content=content,
                model=response.model,
                provider=self.provider_name,
                finish_reason=choice.finish_reason,
                usage=TokenUsage(
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                ),
                tool_calls=message.tool_calls if hasattr(message, 'tool_calls') else None,
                raw_response=response
            )
            
            self._log_response(result)
            self.logger.info(f"DashScopeè¯·æ±‚æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}s")
            
            return result
            
        except AuthenticationError as e:
            raise LLMAuthenticationError(
                provider=self.provider_name,
                message="APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ",
                original_error=e
            )
        except RateLimitError as e:
            raise LLMRateLimitError(
                provider=self.provider_name,
                message="è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•",
                original_error=e
            )
        except APIError as e:
            # å¢å¼ºé”™è¯¯æ—¥å¿—ï¼šæ‰“å°å¯¼è‡´é”™è¯¯çš„æ¶ˆæ¯åºåˆ—
            self.logger.error(f"âŒ DashScope APIé”™è¯¯: {str(e)}")
            self.logger.error(f"è¯·æ±‚çš„æ¶ˆæ¯åºåˆ—:")
            for i, msg in enumerate(params.get('messages', [])):
                role = msg.get('role', 'unknown')
                has_tool_calls = 'tool_calls' in msg
                has_tool_call_id = 'tool_call_id' in msg
                content = msg.get('content', '')[:100] if msg.get('content') else '(ç©º)'
                
                log_msg = f"  [{i+1}] {role}"
                if has_tool_calls:
                    tool_calls_info = []
                    for tc in msg.get('tool_calls', []):
                        tc_id = tc.get('id', 'no-id')
                        tc_func = tc.get('function', {}).get('name', 'unknown')
                        tool_calls_info.append(f"{tc_func}(id={tc_id})")
                    log_msg += f" + tool_calls=[{', '.join(tool_calls_info)}]"
                elif has_tool_call_id:
                    log_msg += f" + tool_call_id={msg['tool_call_id']}"
                else:
                    log_msg += f": {content}"
                
                self.logger.error(log_msg)
            
            raise LLMAPIError(
                provider=self.provider_name,
                status_code=getattr(e, 'status_code', 500),
                message=str(e),
                original_error=e
            )
        except Exception as e:
            raise self._handle_error(e, "DashScopeèŠå¤©å®Œæˆå¤±è´¥")
    
    def stream_completion(self, request: LLMRequest) -> Iterator[StreamChunk]:
        """æµå¼èŠå¤©å®Œæˆ"""
        if not self.is_available():
            raise LLMProviderError(
                provider=self.provider_name,
                message="Providerä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®"
            )
        
        self._log_request(request)
        
        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = request.to_openai_format()
            params["stream"] = True
            if not params.get("model"):
                raise LLMProviderError(
                    provider=self.provider_name,
                    message="å¿…é¡»æŒ‡å®šæ¨¡å‹åç§°"
                )
            
            # è°ƒç”¨æµå¼API
            stream = self.client.chat.completions.create(**params)
            
            for chunk in stream:
                if chunk.choices:
                    choice = chunk.choices[0]
                    delta = choice.delta
                    
                    yield StreamChunk(
                        content=delta.content or "",
                        finish_reason=choice.finish_reason,
                        tool_calls=delta.tool_calls if hasattr(delta, 'tool_calls') else None
                    )
                    
        except Exception as e:
            raise self._handle_error(e, "DashScopeæµå¼å®Œæˆå¤±è´¥")
    
    def embedding(self, texts: List[str], model: Optional[str] = None) -> EmbeddingResponse:
        """
        æ–‡æœ¬å‘é‡åŒ–
        
        æ³¨æ„ï¼šDashScopeçš„embeddingéœ€è¦ä½¿ç”¨ä¸“é—¨çš„endpoint
        è¿™é‡Œæä¾›åŸºæœ¬å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦è°ƒæ•´
        """
        if not self.is_available():
            raise LLMProviderError(
                provider=self.provider_name,
                message="Providerä¸å¯ç”¨"
            )
        
        # DashScope embeddingå®ç°
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„DashScope embedding APIè¿›è¡Œè°ƒæ•´
        raise NotImplementedError("DashScope embeddingåŠŸèƒ½å¾…å®ç°ï¼Œè¯·ä½¿ç”¨Ollamaè¿›è¡Œå‘é‡åŒ–")
    
    def is_available(self) -> bool:
        """æ£€æŸ¥Provideræ˜¯å¦å¯ç”¨"""
        return self._initialized and self.client is not None
